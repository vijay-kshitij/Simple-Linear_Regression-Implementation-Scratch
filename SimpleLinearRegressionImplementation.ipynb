{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "wlddUP6AhRHk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDFVlotn8yqB",
        "outputId": "8ddba830-57b0-442e-ed99-ba62e4f9f105"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11 24 19 15 12 25 11 23 27 15]\n",
            "[4 9 6 5 9 9 4 7 6 3]\n"
          ]
        }
      ],
      "source": [
        "#creating the dataset\n",
        "\n",
        "np.random.seed(42) #fixing the random state\n",
        "\n",
        "X = np.random.randint(5,30,10)  #independent variable\n",
        "print(X)\n",
        "\n",
        "y = np.random.randint(2,10,10)  #dependent variable\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#defining hypothesis and loss function\n",
        "\n",
        "\n",
        "# Hypothesis Function\n",
        "def h(theta0, theta1, X):\n",
        "  return theta0 + theta1*X\n",
        "\n",
        "# Loss Function\n",
        "def J(theta0, theta1, X, y):\n",
        "  return (np.sum((h(theta0,theta1,X) - y)**2))/(len(X)*2)"
      ],
      "metadata": {
        "id": "M9WuN_eZhORF"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#declaring parameters\n",
        "theta0 = 0\n",
        "theta1 = 0\n",
        "\n",
        "#learning rate\n",
        "alpha = 0.0001\n",
        "\n",
        "i = 0\n",
        "\n",
        "# Convergence threshold\n",
        "my_diff = 0.00001\n",
        "\n",
        "# Initial loss value\n",
        "diff = J(theta0, theta1, X, y)\n",
        "print(\"initial Loss: {}\".format(J(theta0, theta1, X, y)))\n",
        "\n",
        "while diff > my_diff:\n",
        "\n",
        "  loss = J(theta0, theta1, X, y)\n",
        "\n",
        "  #calculating gradients\n",
        "  grad0 = (1/len(X)) * np.sum(h(theta0,theta1,X) - y)\n",
        "  grad1 = (1/len(X)) * np.sum(np.matmul(np.transpose(h(theta0,theta1,X) - y),X))\n",
        "\n",
        "  #updating parameters\n",
        "  temp0 = theta0 - (alpha * grad0)\n",
        "  temp1 = theta1 - (alpha * grad1)\n",
        "\n",
        "  theta0 = temp0\n",
        "  theta1 = temp1\n",
        "\n",
        "  i += 1\n",
        "\n",
        "  new_loss = J(theta0, theta1, X, y)\n",
        "  print(f\"Loss after iteration {i} is {new_loss}\")\n",
        "\n",
        "  diff = loss - new_loss\n",
        "  print(\"diff is \", diff)\n",
        "  print()\n",
        "\n",
        "\n",
        "print(\"Final theta0 = \", theta0)\n",
        "print(\"Final theta1 = \", theta1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmvdgkNljpFD",
        "outputId": "9f890da5-655a-4296-a883-cda919fe16ef"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial Loss: 21.5\n",
            "Loss after iteration 1 is 20.09671554924\n",
            "diff is  1.403284450760001\n",
            "\n",
            "Loss after iteration 2 is 18.794408150089275\n",
            "diff is  1.302307399150724\n",
            "\n",
            "Loss after iteration 3 is 17.5858116977281\n",
            "diff is  1.2085964523611743\n",
            "\n",
            "Loss after iteration 4 is 16.46418294157416\n",
            "diff is  1.1216287561539424\n",
            "\n",
            "Loss after iteration 5 is 15.423263861746912\n",
            "diff is  1.0409190798272459\n",
            "\n",
            "Loss after iteration 6 is 14.457246752845643\n",
            "diff is  0.9660171089012692\n",
            "\n",
            "Loss after iteration 7 is 13.560741820228397\n",
            "diff is  0.8965049326172458\n",
            "\n",
            "Loss after iteration 8 is 12.72874710799725\n",
            "diff is  0.8319947122311468\n",
            "\n",
            "Loss after iteration 9 is 11.956620590905064\n",
            "diff is  0.772126517092186\n",
            "\n",
            "Loss after iteration 10 is 11.240054274472346\n",
            "diff is  0.7165663164327185\n",
            "\n",
            "Loss after iteration 11 is 10.575050158807374\n",
            "diff is  0.6650041156649724\n",
            "\n",
            "Loss after iteration 12 is 9.957897932021325\n",
            "diff is  0.6171522267860485\n",
            "\n",
            "Loss after iteration 13 is 9.38515426878027\n",
            "diff is  0.5727436632410559\n",
            "\n",
            "Loss after iteration 14 is 8.853623618491515\n",
            "diff is  0.5315306502887545\n",
            "\n",
            "Loss after iteration 15 is 8.360340375933335\n",
            "diff is  0.4932832425581797\n",
            "\n",
            "Loss after iteration 16 is 7.902552334850178\n",
            "diff is  0.4577880410831572\n",
            "\n",
            "Loss after iteration 17 is 7.477705332193771\n",
            "diff is  0.4248470026564064\n",
            "\n",
            "Loss after iteration 18 is 7.08342899733365\n",
            "diff is  0.3942763348601215\n",
            "\n",
            "Loss after iteration 19 is 6.7175235267257065\n",
            "diff is  0.36590547060794343\n",
            "\n",
            "Loss after iteration 20 is 6.377947410248943\n",
            "diff is  0.33957611647676345\n",
            "\n",
            "Loss after iteration 21 is 6.062806040730215\n",
            "diff is  0.3151413695187282\n",
            "\n",
            "Loss after iteration 22 is 5.770341143104602\n",
            "diff is  0.2924648976256128\n",
            "\n",
            "Loss after iteration 23 is 5.498920964232079\n",
            "diff is  0.271420178872523\n",
            "\n",
            "Loss after iteration 24 is 5.247031168635152\n",
            "diff is  0.25188979559692726\n",
            "\n",
            "Loss after iteration 25 is 5.01326638936085\n",
            "diff is  0.23376477927430184\n",
            "\n",
            "Loss after iteration 26 is 4.796322386825646\n",
            "diff is  0.21694400253520385\n",
            "\n",
            "Loss after iteration 27 is 4.594988771894046\n",
            "diff is  0.20133361493159985\n",
            "\n",
            "Loss after iteration 28 is 4.408142252589778\n",
            "diff is  0.1868465193042681\n",
            "\n",
            "Loss after iteration 29 is 4.234740366760009\n",
            "diff is  0.1734018858297688\n",
            "\n",
            "Loss after iteration 30 is 4.073815665724384\n",
            "diff is  0.16092470103562562\n",
            "\n",
            "Loss after iteration 31 is 3.924470316456952\n",
            "diff is  0.1493453492674317\n",
            "\n",
            "Loss after iteration 32 is 3.785871092184183\n",
            "diff is  0.13859922427276894\n",
            "\n",
            "Loss after iteration 33 is 3.6572447234494447\n",
            "diff is  0.12862636873473843\n",
            "\n",
            "Loss after iteration 34 is 3.537873583705484\n",
            "diff is  0.11937113974396052\n",
            "\n",
            "Loss after iteration 35 is 3.4270916853629787\n",
            "diff is  0.11078189834250551\n",
            "\n",
            "Loss after iteration 36 is 3.324280963955347\n",
            "diff is  0.1028107214076317\n",
            "\n",
            "Loss after iteration 37 is 3.2288678296875686\n",
            "diff is  0.09541313426777842\n",
            "\n",
            "Loss after iteration 38 is 3.1403199671285713\n",
            "diff is  0.08854786255899727\n",
            "\n",
            "Loss after iteration 39 is 3.0581433651913157\n",
            "diff is  0.08217660193725562\n",
            "\n",
            "Loss after iteration 40 is 2.981879560829505\n",
            "diff is  0.0762638043618109\n",
            "\n",
            "Loss after iteration 41 is 2.9111030810723237\n",
            "diff is  0.07077647975718104\n",
            "\n",
            "Loss after iteration 42 is 2.8454190691251875\n",
            "diff is  0.06568401194713624\n",
            "\n",
            "Loss after iteration 43 is 2.7844610812914987\n",
            "diff is  0.060957987833688776\n",
            "\n",
            "Loss after iteration 44 is 2.7278890424234628\n",
            "diff is  0.05657203886803597\n",
            "\n",
            "Loss after iteration 45 is 2.675387348494534\n",
            "diff is  0.052501693928928894\n",
            "\n",
            "Loss after iteration 46 is 2.626663105706921\n",
            "diff is  0.04872424278761267\n",
            "\n",
            "Loss after iteration 47 is 2.581444496309367\n",
            "diff is  0.0452186093975544\n",
            "\n",
            "Loss after iteration 48 is 2.5394792620073727\n",
            "diff is  0.04196523430199406\n",
            "\n",
            "Loss after iteration 49 is 2.5005332965041727\n",
            "diff is  0.038945965503200064\n",
            "\n",
            "Loss after iteration 50 is 2.464389339319614\n",
            "diff is  0.03614395718455876\n",
            "\n",
            "Loss after iteration 51 is 2.430845763599183\n",
            "diff is  0.03354357572043076\n",
            "\n",
            "Loss after iteration 52 is 2.3997154511498775\n",
            "diff is  0.03113031244930564\n",
            "\n",
            "Loss after iteration 53 is 2.3708247484261866\n",
            "diff is  0.028890702723690875\n",
            "\n",
            "Loss after iteration 54 is 2.3440124976412493\n",
            "diff is  0.02681225078493732\n",
            "\n",
            "Loss after iteration 55 is 2.3191291375972765\n",
            "diff is  0.024883360043972846\n",
            "\n",
            "Loss after iteration 56 is 2.296035869218405\n",
            "diff is  0.02309326837887138\n",
            "\n",
            "Loss after iteration 57 is 2.2746038811301292\n",
            "diff is  0.021431988088275844\n",
            "\n",
            "Loss after iteration 58 is 2.2547136309644693\n",
            "diff is  0.01989025016565993\n",
            "\n",
            "Loss after iteration 59 is 2.236254178380967\n",
            "diff is  0.018459452583502234\n",
            "\n",
            "Loss after iteration 60 is 2.219122566082149\n",
            "diff is  0.017131612298817878\n",
            "\n",
            "Loss after iteration 61 is 2.203223245369867\n",
            "diff is  0.015899320712282172\n",
            "\n",
            "Loss after iteration 62 is 2.1884675430374414\n",
            "diff is  0.014755702332425624\n",
            "\n",
            "Loss after iteration 63 is 2.1747731666231727\n",
            "diff is  0.013694376414268739\n",
            "\n",
            "Loss after iteration 64 is 2.1620637452648124\n",
            "diff is  0.012709421358360284\n",
            "\n",
            "Loss after iteration 65 is 2.1502684035932167\n",
            "diff is  0.011795341671595683\n",
            "\n",
            "Loss after iteration 66 is 2.1393213662877537\n",
            "diff is  0.010947037305462981\n",
            "\n",
            "Loss after iteration 67 is 2.129161591087114\n",
            "diff is  0.010159775200639931\n",
            "\n",
            "Loss after iteration 68 is 2.1197324282078966\n",
            "diff is  0.009429162879217134\n",
            "\n",
            "Loss after iteration 69 is 2.110981304270787\n",
            "diff is  0.00875112393710964\n",
            "\n",
            "Loss after iteration 70 is 2.102859428970729\n",
            "diff is  0.008121875300057813\n",
            "\n",
            "Loss after iteration 71 is 2.095321522854545\n",
            "diff is  0.00753790611618399\n",
            "\n",
            "Loss after iteration 72 is 2.088325564687119\n",
            "diff is  0.006995958167426153\n",
            "\n",
            "Loss after iteration 73 is 2.0818325569965994\n",
            "diff is  0.0064930076905196366\n",
            "\n",
            "Loss after iteration 74 is 2.075806308490501\n",
            "diff is  0.006026248506098231\n",
            "\n",
            "Loss after iteration 75 is 2.0702132321287\n",
            "diff is  0.005593076361801064\n",
            "\n",
            "Loss after iteration 76 is 2.0650221577266867\n",
            "diff is  0.005191074402013385\n",
            "\n",
            "Loss after iteration 77 is 2.0602041580435184\n",
            "diff is  0.004817999683168317\n",
            "\n",
            "Loss after iteration 78 is 2.0557323873841256\n",
            "diff is  0.00447177065939286\n",
            "\n",
            "Loss after iteration 79 is 2.0515819318154747\n",
            "diff is  0.0041504555686509015\n",
            "\n",
            "Loss after iteration 80 is 2.0477296701608654\n",
            "diff is  0.003852261654609279\n",
            "\n",
            "Loss after iteration 81 is 2.0441541449968073\n",
            "diff is  0.0035755251640581243\n",
            "\n",
            "Loss after iteration 82 is 2.0408354429326936\n",
            "diff is  0.0033187020641136655\n",
            "\n",
            "Loss after iteration 83 is 2.037755083505319\n",
            "diff is  0.0030803594273747237\n",
            "\n",
            "Loss after iteration 84 is 2.0348959160683053\n",
            "diff is  0.0028591674370135323\n",
            "\n",
            "Loss after iteration 85 is 2.032242024101175\n",
            "diff is  0.002653891967130395\n",
            "\n",
            "Loss after iteration 86 is 2.0297786364041377\n",
            "diff is  0.002463387697037245\n",
            "\n",
            "Loss after iteration 87 is 2.0274920446831266\n",
            "diff is  0.0022865917210110887\n",
            "\n",
            "Loss after iteration 88 is 2.0253695270652483\n",
            "diff is  0.0021225176178782945\n",
            "\n",
            "Loss after iteration 89 is 2.023399277117905\n",
            "diff is  0.0019702499473432944\n",
            "\n",
            "Loss after iteration 90 is 2.02157033797556\n",
            "diff is  0.001828939142344943\n",
            "\n",
            "Loss after iteration 91 is 2.019872541206606\n",
            "diff is  0.0016977967689539852\n",
            "\n",
            "Loss after iteration 92 is 2.0182964500792493\n",
            "diff is  0.0015760911273567935\n",
            "\n",
            "Loss after iteration 93 is 2.016833306909871\n",
            "diff is  0.0014631431693783448\n",
            "\n",
            "Loss after iteration 94 is 2.0154749842000887\n",
            "diff is  0.0013583227097822004\n",
            "\n",
            "Loss after iteration 95 is 2.0142139392898994\n",
            "diff is  0.0012610449101893018\n",
            "\n",
            "Loss after iteration 96 is 2.01304317227389\n",
            "diff is  0.0011707670160094885\n",
            "\n",
            "Loss after iteration 97 is 2.0119561869457128\n",
            "diff is  0.0010869853281771924\n",
            "\n",
            "Loss after iteration 98 is 2.0109469545529164\n",
            "diff is  0.0010092323927963776\n",
            "\n",
            "Loss after iteration 99 is 2.0100098801599096\n",
            "diff is  0.0009370743930068315\n",
            "\n",
            "Loss after iteration 100 is 2.009139771431372\n",
            "diff is  0.0008701087285376552\n",
            "\n",
            "Loss after iteration 101 is 2.0083318096619545\n",
            "diff is  0.0008079617694174424\n",
            "\n",
            "Loss after iteration 102 is 2.0075815228906153\n",
            "diff is  0.0007502867713391481\n",
            "\n",
            "Loss after iteration 103 is 2.0068847609496077\n",
            "diff is  0.0006967619410076509\n",
            "\n",
            "Loss after iteration 104 is 2.006237672308877\n",
            "diff is  0.0006470886407305976\n",
            "\n",
            "Loss after iteration 105 is 2.005636682586698\n",
            "diff is  0.0006009897221792571\n",
            "\n",
            "Loss after iteration 106 is 2.00507847460664\n",
            "diff is  0.000558207980057901\n",
            "\n",
            "Loss after iteration 107 is 2.0045599698895904\n",
            "diff is  0.0005185047170495061\n",
            "\n",
            "Loss after iteration 108 is 2.0040783114775853\n",
            "diff is  0.00048165841200509263\n",
            "\n",
            "Loss after iteration 109 is 2.003630847993586\n",
            "diff is  0.00044746348399948843\n",
            "\n",
            "Loss after iteration 110 is 2.003215118848293\n",
            "diff is  0.00041572914529286464\n",
            "\n",
            "Loss after iteration 111 is 2.0028288405114485\n",
            "diff is  0.0003862783368444589\n",
            "\n",
            "Loss after iteration 112 is 2.0024698937710226\n",
            "diff is  0.0003589467404259139\n",
            "\n",
            "Loss after iteration 113 is 2.0021363119092044\n",
            "diff is  0.0003335818618181996\n",
            "\n",
            "Loss after iteration 114 is 2.0018262697292295\n",
            "diff is  0.0003100421799748787\n",
            "\n",
            "Loss after iteration 115 is 2.001538073371805\n",
            "diff is  0.0002881963574243862\n",
            "\n",
            "Loss after iteration 116 is 2.001270150864336\n",
            "diff is  0.0002679225074690983\n",
            "\n",
            "Loss after iteration 117 is 2.0010210433502045\n",
            "diff is  0.0002491075141315413\n",
            "\n",
            "Loss after iteration 118 is 2.0007893969491657\n",
            "diff is  0.00023164640103878753\n",
            "\n",
            "Loss after iteration 119 is 2.000573955203468\n",
            "diff is  0.00021544174569765318\n",
            "\n",
            "Loss after iteration 120 is 2.000373552067521\n",
            "diff is  0.00020040313594682502\n",
            "\n",
            "Loss after iteration 121 is 2.0001871054020253\n",
            "diff is  0.0001864466654959429\n",
            "\n",
            "Loss after iteration 122 is 2.0000136109362368\n",
            "diff is  0.00017349446578851513\n",
            "\n",
            "Loss after iteration 123 is 1.9998521366647068\n",
            "diff is  0.00016147427152990446\n",
            "\n",
            "Loss after iteration 124 is 1.9997018176472228\n",
            "diff is  0.0001503190174840796\n",
            "\n",
            "Loss after iteration 125 is 1.999561851182919\n",
            "diff is  0.000139966464303809\n",
            "\n",
            "Loss after iteration 126 is 1.9994314923316654\n",
            "diff is  0.00013035885125356472\n",
            "\n",
            "Loss after iteration 127 is 1.9993100497577199\n",
            "diff is  0.00012144257394552938\n",
            "\n",
            "Loss after iteration 128 is 1.9991968818724661\n",
            "diff is  0.00011316788525372878\n",
            "\n",
            "Loss after iteration 129 is 1.9990913932547074\n",
            "diff is  0.00010548861775871998\n",
            "\n",
            "Loss after iteration 130 is 1.998993031328554\n",
            "diff is  9.836192615342298e-05\n",
            "\n",
            "Loss after iteration 131 is 1.998901283280364\n",
            "diff is  9.174804818989912e-05\n",
            "\n",
            "Loss after iteration 132 is 1.9988156731975308\n",
            "diff is  8.5610082833254e-05\n",
            "\n",
            "Loss after iteration 133 is 1.9987357594131534\n",
            "diff is  7.99137843774389e-05\n",
            "\n",
            "Loss after iteration 134 is 1.998661132041795\n",
            "diff is  7.462737135832676e-05\n",
            "\n",
            "Loss after iteration 135 is 1.998591410692543\n",
            "diff is  6.972134925198326e-05\n",
            "\n",
            "Loss after iteration 136 is 1.9985262423466517\n",
            "diff is  6.516834589143095e-05\n",
            "\n",
            "Loss after iteration 137 is 1.9984652993878858\n",
            "diff is  6.094295876590827e-05\n",
            "\n",
            "Loss after iteration 138 is 1.9984082777746153\n",
            "diff is  5.702161327048039e-05\n",
            "\n",
            "Loss after iteration 139 is 1.9983548953434365\n",
            "diff is  5.338243117880559e-05\n",
            "\n",
            "Loss after iteration 140 is 1.9983048902348468\n",
            "diff is  5.00051085896569e-05\n",
            "\n",
            "Loss after iteration 141 is 1.998258019432231\n",
            "diff is  4.68708026157838e-05\n",
            "\n",
            "Loss after iteration 142 is 1.998214057405956\n",
            "diff is  4.3962026275101707e-05\n",
            "\n",
            "Loss after iteration 143 is 1.998172794855046\n",
            "diff is  4.126255090985964e-05\n",
            "\n",
            "Loss after iteration 144 is 1.9981340375394023\n",
            "diff is  3.875731564373375e-05\n",
            "\n",
            "Loss after iteration 145 is 1.9980976051960482\n",
            "diff is  3.6432343354153574e-05\n",
            "\n",
            "Loss after iteration 146 is 1.998063330533369\n",
            "diff is  3.427466267913459e-05\n",
            "\n",
            "Loss after iteration 147 is 1.9980310582977185\n",
            "diff is  3.227223565049897e-05\n",
            "\n",
            "Loss after iteration 148 is 1.9980006444071872\n",
            "diff is  3.0413890531377774e-05\n",
            "\n",
            "Loss after iteration 149 is 1.9979719551477078\n",
            "diff is  2.868925947940859e-05\n",
            "\n",
            "Loss after iteration 150 is 1.9979448664270099\n",
            "diff is  2.7088720697898694e-05\n",
            "\n",
            "Loss after iteration 151 is 1.9979192630822495\n",
            "diff is  2.560334476031656e-05\n",
            "\n",
            "Loss after iteration 152 is 1.997895038237472\n",
            "diff is  2.422484477748732e-05\n",
            "\n",
            "Loss after iteration 153 is 1.9978720927072953\n",
            "diff is  2.29455301767878e-05\n",
            "\n",
            "Loss after iteration 154 is 1.9978503344435228\n",
            "diff is  2.1758263772486686e-05\n",
            "\n",
            "Loss after iteration 155 is 1.997829678021556\n",
            "diff is  2.0656421966691596e-05\n",
            "\n",
            "Loss after iteration 156 is 1.99781004416378\n",
            "diff is  1.9633857776035768e-05\n",
            "\n",
            "Loss after iteration 157 is 1.9977913592972363\n",
            "diff is  1.8684866543772216e-05\n",
            "\n",
            "Loss after iteration 158 is 1.9977735551431284\n",
            "diff is  1.780415410790326e-05\n",
            "\n",
            "Loss after iteration 159 is 1.9977565683358616\n",
            "diff is  1.698680726680557e-05\n",
            "\n",
            "Loss after iteration 160 is 1.9977403400695046\n",
            "diff is  1.622826635694352e-05\n",
            "\n",
            "Loss after iteration 161 is 1.9977248157696805\n",
            "diff is  1.5524299824098975e-05\n",
            "\n",
            "Loss after iteration 162 is 1.9977099447890745\n",
            "diff is  1.4870980606040973e-05\n",
            "\n",
            "Loss after iteration 163 is 1.997695680124849\n",
            "diff is  1.4264664225382973e-05\n",
            "\n",
            "Loss after iteration 164 is 1.9976819781563986\n",
            "diff is  1.3701968450519075e-05\n",
            "\n",
            "Loss after iteration 165 is 1.9976687984019645\n",
            "diff is  1.3179754434045066e-05\n",
            "\n",
            "Loss after iteration 166 is 1.9976561032927769\n",
            "diff is  1.269510918766592e-05\n",
            "\n",
            "Loss after iteration 167 is 1.9976438579634366\n",
            "diff is  1.2245329340299094e-05\n",
            "\n",
            "Loss after iteration 168 is 1.9976320300573889\n",
            "diff is  1.182790604770112e-05\n",
            "\n",
            "Loss after iteration 169 is 1.9976205895463903\n",
            "diff is  1.144051099855048e-05\n",
            "\n",
            "Loss after iteration 170 is 1.997609508562969\n",
            "diff is  1.1080983421285495e-05\n",
            "\n",
            "Loss after iteration 171 is 1.997598761244943\n",
            "diff is  1.0747318025972064e-05\n",
            "\n",
            "Loss after iteration 172 is 1.997588323591117\n",
            "diff is  1.0437653826134152e-05\n",
            "\n",
            "Loss after iteration 173 is 1.9975781733273734\n",
            "diff is  1.0150263743513577e-05\n",
            "\n",
            "Loss after iteration 174 is 1.9975682897823972\n",
            "diff is  9.883544976219127e-06\n",
            "\n",
            "Final theta0 =  0.020614614104471178\n",
            "Final theta1 =  0.3251022219697322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S4xxPQ3GmwI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "VFHICIVHHhGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Total 174 iterations are run and the Final Loss is approx. 1.99757 and the values of final parameters came out to be as follows:\n",
        "\n",
        "Final theta0 = 0.020614614104471178\n",
        "\n",
        "Final theta1 = 0.3251022219697322"
      ],
      "metadata": {
        "id": "42dYlsOLG57F"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Pl-TA1DHeiP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}